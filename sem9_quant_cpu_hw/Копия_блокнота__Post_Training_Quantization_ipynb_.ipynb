{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ypQKMC0QIjA",
        "outputId": "a7e7c1ee-98fa-4a1d-e4de-5fe38d946e59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch                         2.9.1\n",
            "torchvision                   0.24.1\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NhEMHX1V4IwM"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoMW3Ruptax1"
      },
      "source": [
        "*   https://pytorch.org/blog/quantization-in-practice/\n",
        "*   https://pytorch.org/docs/stable/quantization.html\n",
        "*   https://pytorch.org/docs/stable/quantization-support.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbqyoRrl7A0y"
      },
      "source": [
        "### Mapping function and Quantization Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XbgzwD7M1KW8"
      },
      "outputs": [],
      "source": [
        "# r - float tensor, r' - int tensor\n",
        "# S [scaling factor] = (beta - alpha) / (beta_q - alpha_q)\n",
        "# Z [zero point] =  -(alpha / S - alpha_q)\n",
        "\n",
        "def quantize(float_tensor, scale, z):\n",
        "  # Q(r) = round(r/S + Z)\n",
        "  return torch.round(float_tensor / scale + z)\n",
        "\n",
        "def dequantize(int_tensor, scale, z):\n",
        "  # r' = (Q(r) - Z) * S\n",
        "  return (int_tensor - z) * scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBAf6XZi1KXs",
        "outputId": "72b2cd25-f8b7-475f-91bb-d70eb8176083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1880, -0.0844,  1.4646,  0.1828],\n",
            "        [ 0.0944, -0.2456,  0.3274,  0.1167],\n",
            "        [-0.4556, -0.6780, -0.3045, -0.3583]])\n"
          ]
        }
      ],
      "source": [
        "from torch.ao.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver\n",
        "\n",
        "C, L = 3, 4\n",
        "normal = torch.distributions.normal.Normal(0,1)\n",
        "inputs = normal.sample((C, L))\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPanD_eQ4TZl",
        "outputId": "bf92dec6-8d27-46fa-80f4-4651716aeda8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MinMaxObserver (tensor([0.0084]), tensor([81], dtype=torch.int32))\n",
            "MovingAverageMinMaxObserver (tensor([0.0084]), tensor([81], dtype=torch.int32))\n",
            "HistogramObserver (tensor([0.0084]), tensor([81], dtype=torch.int32))\n"
          ]
        }
      ],
      "source": [
        "observers = [MinMaxObserver(), MovingAverageMinMaxObserver(), HistogramObserver()]\n",
        "for obs in observers:\n",
        "  obs(inputs)\n",
        "  print(obs.__class__.__name__, obs.calculate_qparams())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZf3OgNQ49Aj",
        "outputId": "516cb125-515c-40b5-a42e-18e56de9aad8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0032, 0.0004, 0.0026, 0.0021],\n",
              "        [0.0020, 0.0019, 0.0003, 0.0010],\n",
              "        [0.0019, 0.0026, 0.0020, 0.0030]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scale, z = observers[0].calculate_qparams()\n",
        "reconstruction_error = torch.abs(dequantize(quantize(inputs, scale, z), scale, z) - inputs)\n",
        "reconstruction_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb5qUkHP8sUE"
      },
      "source": [
        "### Affine and Symmetric Quantization Schemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x8cv_y6f8tT3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def get_symmetric_range(x):\n",
        "  beta = torch.max(x.max(), x.min().abs())\n",
        "  return -beta.item(), beta.item()\n",
        "\n",
        "def get_affine_range(x):\n",
        "  return x.min().item(), x.max().item()\n",
        "\n",
        "def plot(plt, data, scheme):\n",
        "  boundaries = get_affine_range(data) if scheme == 'affine' else get_symmetric_range(data)\n",
        "  a, _, _ = plt.hist(data, density=True, bins=100)\n",
        "  ymin, ymax = np.quantile(a[a>0], [0.25, 0.95])\n",
        "  plt.vlines(x=boundaries, ls='--', colors='purple', ymin=ymin, ymax=ymax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "NTMv0kcB8128",
        "outputId": "54a6b5a6-d814-4332-af0d-5e36fc8ab74e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "act =  torch.distributions.pareto.Pareto(1, 10).sample((1, 1024))\n",
        "weights = torch.distributions.normal.Normal(0, 0.12).sample((3, 64, 7, 7)).flatten()\n",
        "\n",
        "fig, axs = plt.subplots(2,2)\n",
        "plot(axs[0, 0], act, 'affine')\n",
        "axs[0, 0].set_title(\"Activation, Affine-Quantized\")\n",
        "\n",
        "plot(axs[0, 1], act, 'symmetric')\n",
        "axs[0, 1].set_title(\"Activation, Symmetric-Quantized\")\n",
        "\n",
        "plot(axs[1, 0], weights, 'affine')\n",
        "axs[1, 0].set_title(\"Weights, Affine-Quantized\")\n",
        "\n",
        "plot(axs[1, 1], weights, 'symmetric')\n",
        "axs[1, 1].set_title(\"Weights, Symmetric-Quantized\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bej5FWvR-UDK",
        "outputId": "eb75a699-1968-4940-c10e-332e0a165f59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qscheme: torch.per_tensor_affine | (tensor([0.0084]), tensor([81], dtype=torch.int32))\n",
            "Qscheme: torch.per_tensor_symmetric | (tensor([0.0115]), tensor([128]))\n"
          ]
        }
      ],
      "source": [
        "for qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:\n",
        "  obs = MovingAverageMinMaxObserver(qscheme=qscheme)\n",
        "  obs(inputs)\n",
        "  print(f\"Qscheme: {qscheme} | {obs.calculate_qparams()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc4Zd5M99fTo"
      },
      "source": [
        "### Per-Tensor and Per-Channel Quantization Schemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dltyo4q9l-4",
        "outputId": "b956d032-6a21-4903-fff4-4dcd1f612ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([0.0065, 0.0022, 0.0027]), tensor([ 29, 109, 255], dtype=torch.int32))\n"
          ]
        }
      ],
      "source": [
        "from torch.ao.quantization.observer import MovingAveragePerChannelMinMaxObserver\n",
        "\n",
        "obs = MovingAveragePerChannelMinMaxObserver(ch_axis = 0)  # calculate qparams for all `C` channels separately\n",
        "obs(inputs)\n",
        "print(obs.calculate_qparams())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOd8W97_AAMW"
      },
      "source": [
        "For weights quantization, symmetric-per-channel quantization provides better accuracies; per-tensor quantization performs poorly, possibly due to high variance in conv weights across channels from batchnorm folding.\n",
        "[https://arxiv.org/abs/2004.09602]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvWTOGgv9gmI"
      },
      "source": [
        "https://github.com/pytorch/pytorch/blob/748d9d24940cd17938df963456c90fa1a13f3932/torch/ao/quantization/observer.py#L258"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiBMYYJMBC0A"
      },
      "source": [
        "### Backend Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9OgF7U86BDw7"
      },
      "outputs": [],
      "source": [
        "backend = 'fbgemm' #'fbgemm' if x86 else 'qnnpack'\n",
        "qconfig = torch.ao.quantization.get_default_qconfig(backend)\n",
        "torch.backends.quantized.engine = backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atxxYA28Bcvz"
      },
      "source": [
        "GPUs - via TensorRT and cuDNN\n",
        "https://pytorch.org/docs/stable/quantization.html#note-for-native-cpu-backends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk-uxMCWBpRl"
      },
      "source": [
        "### QConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3xygBpkBqKW",
        "outputId": "32021fdc-0cc3-401d-81f0-a631e2e48864"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, qscheme=torch.per_tensor_affine){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, qscheme=torch.qint8){})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_qconfig = torch.ao.quantization.QConfig(\n",
        "  activation=MovingAverageMinMaxObserver.with_args(qscheme=torch.per_tensor_affine),\n",
        "  weight=MovingAveragePerChannelMinMaxObserver.with_args(qscheme=torch.qint8))\n",
        "\n",
        "my_qconfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mh00mEaV4wy"
      },
      "source": [
        "### Eager Mode v/s FX Graph Mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLkEva02V_Co"
      },
      "source": [
        "*    Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.\n",
        "*    FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently its a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWjnQot3Wzg-"
      },
      "source": [
        "FX Graph Mode automatically fuses eligible modules, inserts Quant/DeQuant stubs, calibrates the model and returns a quantized module - all in two method calls - but only for networks that are symbolic traceable. The examples below contain the calls using Eager Mode and FX Graph Mode for comparison.\n",
        "https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el4bJKgIV6y7"
      },
      "outputs": [],
      "source": [
        "def f(a, b):\n",
        "    if b == True:\n",
        "        return a\n",
        "    else:\n",
        "        return a * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkVCexZefVIX"
      },
      "outputs": [],
      "source": [
        "f = torch.fx.symbolic_trace(f, concrete_args={'b': False})\n",
        "assert f(3, False) == 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Temh_oTBfjE8"
      },
      "source": [
        "https://pytorch.org/docs/stable/fx.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfxsmoBtXOgb"
      },
      "source": [
        "### Post-Training Dynamic/Weight-only Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APgH-cqYdx6O"
      },
      "source": [
        "Here the models weights are pre-quantized; the activations are quantized on-the-fly (dynamic) during inference. The simplest of all approaches, it has a one line API call in torch.quantization.quantize_dynamic. Currently only Linear and Recurrent (LSTM, GRU, RNN) layers are supported for dynamic quantization\n",
        "\n",
        "\n",
        "\n",
        "*   Can result in higher accuracies since the clipping range is exactly calibrated for each input\n",
        "*   Calibrating and quantizing the activations at each layer during runtime can add to the compute overhead\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqIHv8efWt2l",
        "outputId": "d7967b1b-faf4-4258-85c1-f90a69edf34c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(2, 64, kernel_size=(8,), stride=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=16, out_features=10, bias=True)\n",
              "  (3): LSTM(10, 10)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "# toy model\n",
        "m = nn.Sequential(\n",
        "  nn.Conv2d(2, 64, (8,)),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(16, 10),\n",
        "  nn.LSTM(10, 10))\n",
        "\n",
        "m.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jhhTCrvXYv4",
        "outputId": "db761a8e-7441-46f8-899d-c9bc145310ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2181302342.py:4: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = quantize_dynamic(\n"
          ]
        }
      ],
      "source": [
        "## EAGER MODE\n",
        "from torch.ao.quantization import quantize_dynamic\n",
        "\n",
        "model_quantized = quantize_dynamic(\n",
        "    model=m, qconfig_spec={nn.LSTM, nn.Linear}, dtype=torch.qint8, inplace=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC5aJPPgXaO_",
        "outputId": "952108eb-61a2-40db-f741-7efb7100cb61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-15622409.py:6: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/tmp/ipython-input-15622409.py:7: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = quantize_fx.convert_fx(model_prepared)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/nn/quantized/reference/modules/rnn.py:461: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/nn/quantized/reference/modules/rnn.py:467: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(\n"
          ]
        }
      ],
      "source": [
        "## FX MODE\n",
        "from torch.ao.quantization import quantize_fx\n",
        "\n",
        "example_inputs = (torch.randn(1, 3, 224, 224),)\n",
        "qconfig_dict = {\"\": torch.ao.quantization.default_dynamic_qconfig}  # An empty key denotes the default applied to all modules\n",
        "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
        "model_quantized = quantize_fx.convert_fx(model_prepared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_Dd8gxLaNmy"
      },
      "source": [
        "### Post-Training Static Quantization (PTQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCZcZdAIdTzg"
      },
      "source": [
        "PTQ also pre-quantizes model weights but instead of calibrating activations on-the-fly, the clipping range is pre-calibrated and fixed (static) using validation data. Activations stay in quantized precision between operations during inference. About 100 mini-batches of representative data are sufficient to calibrate the observers\n",
        "\n",
        "*   Static quantization has faster inference than dynamic quantization because it eliminates the float<->int conversion costs between layers\n",
        "*   Static quantized models may need regular re-calibration to stay robust against distribution-drift\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v3vGj4WaQYV"
      },
      "outputs": [],
      "source": [
        "# Static quantization of a model consists of the following steps:\n",
        "\n",
        "#     Fuse modules\n",
        "#     Insert Quant/DeQuant Stubs\n",
        "#     Prepare the fused module (insert observers before and after layers)\n",
        "#     Calibrate the prepared module (pass it representative data)\n",
        "#     Convert the calibrated module (replace with quantized version)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import copy\n",
        "\n",
        "backend = \"fbgemm\"  # running on a x86 CPU. Use \"qnnpack\" if running on ARM.\n",
        "\n",
        "model = nn.Sequential(\n",
        "     nn.Conv2d(2, 64, 3),\n",
        "     nn.ReLU(),\n",
        "     nn.Conv2d(64, 128, 3),\n",
        "     nn.ReLU()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCMLt7GaN1K3",
        "outputId": "a6a66d82-6fc9-4cf3-fac7-10270b915196"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[-0.1928, -0.1054,  0.2114],\n",
              "          [-0.0647,  0.2293, -0.1385],\n",
              "          [-0.1778,  0.0213, -0.2144]],\n",
              "\n",
              "         [[ 0.0457,  0.2161,  0.1104],\n",
              "          [ 0.0996, -0.1576,  0.0593],\n",
              "          [-0.1849,  0.1239,  0.1911]]],\n",
              "\n",
              "\n",
              "        [[[-0.0240, -0.2256, -0.1008],\n",
              "          [-0.0429, -0.0425, -0.2341],\n",
              "          [ 0.0884, -0.1269, -0.2153]],\n",
              "\n",
              "         [[-0.0279, -0.1518, -0.0319],\n",
              "          [-0.1016,  0.1213,  0.2062],\n",
              "          [-0.0347,  0.0668,  0.0152]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1080, -0.1748,  0.1471],\n",
              "          [ 0.1659, -0.1662, -0.2282],\n",
              "          [ 0.2282,  0.1972,  0.1749]],\n",
              "\n",
              "         [[-0.0635, -0.2079,  0.1717],\n",
              "          [ 0.0424, -0.2030, -0.1405],\n",
              "          [ 0.2111, -0.0697, -0.1182]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[ 0.0694,  0.2069, -0.1728],\n",
              "          [ 0.1395,  0.2033,  0.1809],\n",
              "          [ 0.0878, -0.2317, -0.1280]],\n",
              "\n",
              "         [[-0.0883,  0.1499,  0.0371],\n",
              "          [-0.0213,  0.1029,  0.1026],\n",
              "          [-0.0526,  0.1986, -0.2077]]],\n",
              "\n",
              "\n",
              "        [[[ 0.2255,  0.1444, -0.1647],\n",
              "          [ 0.1387,  0.1781,  0.1628],\n",
              "          [ 0.1430, -0.0779,  0.2102]],\n",
              "\n",
              "         [[ 0.2031, -0.1700,  0.0088],\n",
              "          [ 0.2080, -0.0611,  0.0901],\n",
              "          [-0.1505,  0.1410, -0.1258]]],\n",
              "\n",
              "\n",
              "        [[[-0.0551, -0.2079,  0.0960],\n",
              "          [-0.0851,  0.0825, -0.1403],\n",
              "          [ 0.1585,  0.1052, -0.1805]],\n",
              "\n",
              "         [[-0.2218,  0.2239, -0.0324],\n",
              "          [-0.0335, -0.1406,  0.1817],\n",
              "          [ 0.2263,  0.2228, -0.1190]]]], requires_grad=True)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model[0].weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgNoxdlneyuM",
        "outputId": "6d4fa1ff-597e-43e1-fbcd-6ee7673eb241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-213743822.py:18: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  torch.ao.quantization.prepare(m, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-213743822.py:29: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  torch.ao.quantization.convert(m, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "## EAGER MODE\n",
        "m = copy.deepcopy(model)\n",
        "m.eval()\n",
        "\n",
        "\"\"\"Fuse\n",
        "- Inplace fusion replaces the first module in the sequence with the fused module, and the rest with identity modules\n",
        "\"\"\"\n",
        "torch.ao.quantization.fuse_modules(m, ['0','1'], inplace=True) # fuse first Conv-ReLU pair\n",
        "torch.ao.quantization.fuse_modules(m, ['2','3'], inplace=True) # fuse second Conv-ReLU pair\n",
        "\n",
        "\"\"\"Insert stubs\"\"\"\n",
        "m = nn.Sequential(torch.ao.quantization.QuantStub(),\n",
        "                  *m,\n",
        "                  torch.ao.quantization.DeQuantStub())\n",
        "\n",
        "\"\"\"Prepare\"\"\"\n",
        "m.qconfig = torch.ao.quantization.get_default_qconfig(backend)\n",
        "torch.ao.quantization.prepare(m, inplace=True)\n",
        "\n",
        "\"\"\"Calibrate\n",
        "- This example uses random data for convenience. Use representative (validation) data instead.\n",
        "\"\"\"\n",
        "with torch.inference_mode():\n",
        "  for _ in range(10):\n",
        "    x = torch.rand(1, 2, 28, 28)\n",
        "    m(x)\n",
        "\n",
        "\"\"\"Convert\"\"\"\n",
        "torch.ao.quantization.convert(m, inplace=True)\n",
        "\n",
        "\"\"\"Check\"\"\"\n",
        "print(m[1].weight().element_size()) # 1 byte instead of 4 bytes for FP32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmyPkO1Bi-Tw",
        "outputId": "411d88e2-79b5-48d7-a85d-bd25460099a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[-0.1924, -0.1061,  0.2122],\n",
              "          [-0.0647,  0.2284, -0.1385],\n",
              "          [-0.1780,  0.0216, -0.2140]],\n",
              "\n",
              "         [[ 0.0450,  0.2158,  0.1097],\n",
              "          [ 0.0989, -0.1582,  0.0593],\n",
              "          [-0.1852,  0.1241,  0.1906]]],\n",
              "\n",
              "\n",
              "        [[[-0.0239, -0.2258, -0.1010],\n",
              "          [-0.0422, -0.0422, -0.2350],\n",
              "          [ 0.0881, -0.1267, -0.2148]],\n",
              "\n",
              "         [[-0.0275, -0.1524, -0.0312],\n",
              "          [-0.1010,  0.1212,  0.2056],\n",
              "          [-0.0349,  0.0661,  0.0147]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1074, -0.1754,  0.1468],\n",
              "          [ 0.1665, -0.1665, -0.2291],\n",
              "          [ 0.2273,  0.1969,  0.1754]],\n",
              "\n",
              "         [[-0.0626, -0.2076,  0.1718],\n",
              "          [ 0.0430, -0.2023, -0.1396],\n",
              "          [ 0.2112, -0.0698, -0.1181]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[ 0.0691,  0.2072, -0.1727],\n",
              "          [ 0.1399,  0.2036,  0.1817],\n",
              "          [ 0.0872, -0.2326, -0.1272]],\n",
              "\n",
              "         [[-0.0891,  0.1490,  0.0363],\n",
              "          [-0.0218,  0.1036,  0.1018],\n",
              "          [-0.0527,  0.1981, -0.2072]]],\n",
              "\n",
              "\n",
              "        [[[ 0.2247,  0.1451, -0.1645],\n",
              "          [ 0.1380,  0.1787,  0.1627],\n",
              "          [ 0.1433, -0.0778,  0.2105]],\n",
              "\n",
              "         [[ 0.2034, -0.1698,  0.0088],\n",
              "          [ 0.2087, -0.0619,  0.0902],\n",
              "          [-0.1504,  0.1415, -0.1256]]],\n",
              "\n",
              "\n",
              "        [[[-0.0550, -0.2076,  0.0958],\n",
              "          [-0.0852,  0.0816, -0.1402],\n",
              "          [ 0.1580,  0.1047, -0.1810]],\n",
              "\n",
              "         [[-0.2218,  0.2236, -0.0319],\n",
              "          [-0.0337, -0.1402,  0.1810],\n",
              "          [ 0.2254,  0.2236, -0.1189]]]], size=(64, 2, 3, 3),\n",
              "       dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n",
              "       scale=tensor([0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0017, 0.0018, 0.0016,\n",
              "        0.0018, 0.0018, 0.0018, 0.0017, 0.0016, 0.0018, 0.0018, 0.0016, 0.0018,\n",
              "        0.0017, 0.0015, 0.0016, 0.0017, 0.0017, 0.0018, 0.0017, 0.0018, 0.0018,\n",
              "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0015, 0.0017,\n",
              "        0.0017, 0.0018, 0.0018, 0.0017, 0.0018, 0.0015, 0.0018, 0.0016, 0.0018,\n",
              "        0.0018, 0.0016, 0.0018, 0.0017, 0.0017, 0.0017, 0.0017, 0.0018, 0.0018,\n",
              "        0.0018, 0.0013, 0.0018, 0.0017, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018,\n",
              "        0.0018], dtype=torch.float64),\n",
              "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              "       axis=0)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m[1].weight()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8a-ihuEe13L",
        "outputId": "d70884ff-96c6-4971-8287-4cf24ab74ca6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-750729956.py:10: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/tmp/ipython-input-750729956.py:19: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = quantize_fx.convert_fx(model_prepared)\n"
          ]
        }
      ],
      "source": [
        "## FX GRAPH\n",
        "from torch.ao.quantization import quantize_fx\n",
        "\n",
        "m = copy.deepcopy(model)\n",
        "m.eval()\n",
        "qconfig_dict = {\"\": torch.ao.quantization.get_default_qconfig(backend)}\n",
        "\n",
        "# Prepare\n",
        "example_inputs = (torch.randn(1, 3, 224, 224),)\n",
        "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs)\n",
        "\n",
        "# Calibrate - Use representative (validation) data.\n",
        "with torch.inference_mode():\n",
        "  for _ in range(10):\n",
        "    x = torch.rand(1, 2, 28, 28)\n",
        "    model_prepared(x)\n",
        "\n",
        "# quantize\n",
        "model_quantized = quantize_fx.convert_fx(model_prepared)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmWbt72hO-30",
        "outputId": "9eac0918-86cb-4107-91b0-370553a0f0bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[-0.1928, -0.1054,  0.2114],\n",
              "          [-0.0647,  0.2293, -0.1385],\n",
              "          [-0.1778,  0.0213, -0.2144]],\n",
              "\n",
              "         [[ 0.0457,  0.2161,  0.1104],\n",
              "          [ 0.0996, -0.1576,  0.0593],\n",
              "          [-0.1849,  0.1239,  0.1911]]],\n",
              "\n",
              "\n",
              "        [[[-0.0240, -0.2256, -0.1008],\n",
              "          [-0.0429, -0.0425, -0.2341],\n",
              "          [ 0.0884, -0.1269, -0.2153]],\n",
              "\n",
              "         [[-0.0279, -0.1518, -0.0319],\n",
              "          [-0.1016,  0.1213,  0.2062],\n",
              "          [-0.0347,  0.0668,  0.0152]]],\n",
              "\n",
              "\n",
              "        [[[ 0.1080, -0.1748,  0.1471],\n",
              "          [ 0.1659, -0.1662, -0.2282],\n",
              "          [ 0.2282,  0.1972,  0.1749]],\n",
              "\n",
              "         [[-0.0635, -0.2079,  0.1717],\n",
              "          [ 0.0424, -0.2030, -0.1405],\n",
              "          [ 0.2111, -0.0697, -0.1182]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[ 0.0694,  0.2069, -0.1728],\n",
              "          [ 0.1395,  0.2033,  0.1809],\n",
              "          [ 0.0878, -0.2317, -0.1280]],\n",
              "\n",
              "         [[-0.0883,  0.1499,  0.0371],\n",
              "          [-0.0213,  0.1029,  0.1026],\n",
              "          [-0.0526,  0.1986, -0.2077]]],\n",
              "\n",
              "\n",
              "        [[[ 0.2255,  0.1444, -0.1647],\n",
              "          [ 0.1387,  0.1781,  0.1628],\n",
              "          [ 0.1430, -0.0779,  0.2102]],\n",
              "\n",
              "         [[ 0.2031, -0.1700,  0.0088],\n",
              "          [ 0.2080, -0.0611,  0.0901],\n",
              "          [-0.1505,  0.1410, -0.1258]]],\n",
              "\n",
              "\n",
              "        [[[-0.0551, -0.2079,  0.0960],\n",
              "          [-0.0851,  0.0825, -0.1403],\n",
              "          [ 0.1585,  0.1052, -0.1805]],\n",
              "\n",
              "         [[-0.2218,  0.2239, -0.0324],\n",
              "          [-0.0335, -0.1406,  0.1817],\n",
              "          [ 0.2263,  0.2228, -0.1190]]]], requires_grad=True)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m[0].weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT-qggJdacLr"
      },
      "source": [
        "### SENSITIVITY ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD294D_Tac3K",
        "outputId": "4fffd9c7-c3ab-457e-9665-be812055e737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Only quantizing layer:  \n",
            "Only quantizing layer:  0\n",
            "Only quantizing layer:  1\n",
            "Only quantizing layer:  2\n",
            "Only quantizing layer:  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1147990886.py:11: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_prepared = quantize_fx.prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/tmp/ipython-input-1147990886.py:13: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = quantize_fx.convert_fx(model_prepared)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# ONE-AT-A-TIME SENSITIVITY ANALYSIS\n",
        "\n",
        "for quantized_layer, _ in model.named_modules():\n",
        "  print(\"Only quantizing layer: \", quantized_layer)\n",
        "\n",
        "  # The module_name key allows module-specific qconfigs.\n",
        "  qconfig_dict = {\"\": None,\n",
        "  \"module_name\":[(quantized_layer, torch.quantization.get_default_qconfig(backend))]}\n",
        "\n",
        "  example_inputs = (torch.randn(1, 3, 224, 224),)\n",
        "  model_prepared = quantize_fx.prepare_fx(model, qconfig_dict, example_inputs)\n",
        "  # calibrate\n",
        "  model_quantized = quantize_fx.convert_fx(model_prepared)\n",
        "  # evaluate(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dXAIoKwbCFl"
      },
      "source": [
        " Numeric Suite - https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7FfZtQdbV90"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import torch.ao.quantization\n",
        "import torch.ao.ns._numeric_suite as ns\n",
        "from torch.ao.quantization import (\n",
        "    default_eval_fn,\n",
        "    default_qconfig,\n",
        "    quantize,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm-ESg6obWWp",
        "outputId": "fae1e41b-a824-4a4b-bef5-d91dc996831e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 44.7M/44.7M [00:00<00:00, 202MB/s]\n",
            "/tmp/ipython-input-409614095.py:7: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  qmodel = quantize(float_model, default_eval_fn, [[img_data]], inplace=False)\n"
          ]
        }
      ],
      "source": [
        "float_model = torchvision.models.quantization.resnet18(pretrained=True, quantize=False)\n",
        "float_model.to('cpu')\n",
        "float_model.eval()\n",
        "float_model.fuse_model()\n",
        "float_model.qconfig = torch.quantization.default_qconfig\n",
        "img_data = (torch.rand(2, 3, 10, 10, dtype=torch.float), torch.randint(0, 1, (2,), dtype=torch.long))\n",
        "qmodel = quantize(float_model, default_eval_fn, [[img_data]], inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKZEOpfPbMNu",
        "outputId": "1e8a3db9-d59c-45df-f086-dae84d7c3e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1.weight tensor(31.6638)\n",
            "layer1.0.conv1.weight tensor(30.6450)\n",
            "layer1.0.conv2.weight tensor(31.1528)\n",
            "layer1.1.conv1.weight tensor(32.1438)\n",
            "layer1.1.conv2.weight tensor(31.2477)\n",
            "layer2.0.conv1.weight tensor(30.9890)\n",
            "layer2.0.conv2.weight tensor(28.8233)\n",
            "layer2.0.downsample.0.weight tensor(31.5558)\n",
            "layer2.1.conv1.weight tensor(30.7668)\n",
            "layer2.1.conv2.weight tensor(28.4516)\n",
            "layer3.0.conv1.weight tensor(30.9247)\n",
            "layer3.0.conv2.weight tensor(26.6841)\n",
            "layer3.0.downsample.0.weight tensor(28.7825)\n",
            "layer3.1.conv1.weight tensor(28.9707)\n",
            "layer3.1.conv2.weight tensor(25.6784)\n",
            "layer4.0.conv1.weight tensor(26.8495)\n",
            "layer4.0.conv2.weight tensor(25.8394)\n",
            "layer4.0.downsample.0.weight tensor(28.6355)\n",
            "layer4.1.conv1.weight tensor(26.8758)\n",
            "layer4.1.conv2.weight tensor(28.4319)\n",
            "fc._packed_params._packed_params tensor(32.6505)\n",
            "---\n",
            "conv1.stats tensor(37.4195, grad_fn=<MulBackward0>)\n",
            "layer1.0.conv1.stats tensor(29.9686, grad_fn=<MulBackward0>)\n",
            "layer1.0.conv2.stats tensor(28.8940, grad_fn=<MulBackward0>)\n",
            "layer1.0.add_relu.stats tensor(32.6408, grad_fn=<MulBackward0>)\n",
            "layer1.1.conv1.stats tensor(30.2182, grad_fn=<MulBackward0>)\n",
            "layer1.1.conv2.stats tensor(26.0196, grad_fn=<MulBackward0>)\n",
            "layer1.1.add_relu.stats tensor(29.8501, grad_fn=<MulBackward0>)\n",
            "layer2.0.conv1.stats tensor(26.9117, grad_fn=<MulBackward0>)\n",
            "layer2.0.conv2.stats tensor(25.8995, grad_fn=<MulBackward0>)\n",
            "layer2.0.downsample.0.stats tensor(22.7773, grad_fn=<MulBackward0>)\n",
            "layer2.0.add_relu.stats tensor(25.8997, grad_fn=<MulBackward0>)\n",
            "layer2.1.conv1.stats tensor(25.5431, grad_fn=<MulBackward0>)\n",
            "layer2.1.conv2.stats tensor(24.0907, grad_fn=<MulBackward0>)\n",
            "layer2.1.add_relu.stats tensor(26.0697, grad_fn=<MulBackward0>)\n",
            "layer3.0.conv1.stats tensor(26.6940, grad_fn=<MulBackward0>)\n",
            "layer3.0.conv2.stats tensor(26.7777, grad_fn=<MulBackward0>)\n",
            "layer3.0.downsample.0.stats tensor(25.5981, grad_fn=<MulBackward0>)\n",
            "layer3.0.add_relu.stats tensor(25.2125, grad_fn=<MulBackward0>)\n",
            "layer3.1.conv1.stats tensor(30.9370, grad_fn=<MulBackward0>)\n",
            "layer3.1.conv2.stats tensor(26.2081, grad_fn=<MulBackward0>)\n",
            "layer3.1.add_relu.stats tensor(25.4769, grad_fn=<MulBackward0>)\n",
            "layer4.0.conv1.stats tensor(27.2297, grad_fn=<MulBackward0>)\n",
            "layer4.0.conv2.stats tensor(27.0554, grad_fn=<MulBackward0>)\n",
            "layer4.0.downsample.0.stats tensor(21.6605, grad_fn=<MulBackward0>)\n",
            "layer4.0.add_relu.stats tensor(19.8284, grad_fn=<MulBackward0>)\n",
            "layer4.1.conv1.stats tensor(25.5621, grad_fn=<MulBackward0>)\n",
            "layer4.1.conv2.stats tensor(17.6906, grad_fn=<MulBackward0>)\n",
            "layer4.1.add_relu.stats tensor(17.1031, grad_fn=<MulBackward0>)\n",
            "fc.stats tensor(18.8560, grad_fn=<MulBackward0>)\n",
            "quant.stats tensor(48.2756)\n"
          ]
        }
      ],
      "source": [
        "def compute_error(x, y):\n",
        "    Ps = torch.norm(x)\n",
        "    Pn = torch.norm(x - y)\n",
        "    return 20 * torch.log10(Ps/Pn)\n",
        "\n",
        "wt_compare_dict = ns.compare_weights(float_model.state_dict(), qmodel.state_dict())\n",
        "for key in wt_compare_dict:\n",
        "    print(key, compute_error(wt_compare_dict[key]['float'], wt_compare_dict[key]['quantized'].dequantize()))\n",
        "\n",
        "print(\"---\")\n",
        "\n",
        "act_compare_dict = ns.compare_model_outputs(float_model, qmodel, img_data[0])\n",
        "for key in act_compare_dict:\n",
        "    print(key, compute_error(act_compare_dict[key]['float'][0], act_compare_dict[key]['quantized'][0].dequantize()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "eYUaWoAyeL7a",
        "outputId": "6b27493a-a0fc-42b3-b4cf-16d4ef8a62c5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f = wt_compare_dict['conv1.weight']['float'].flatten()\n",
        "plt.hist(f, bins = 100, label='orig')\n",
        "\n",
        "q = wt_compare_dict['conv1.weight']['quantized'].flatten().dequantize()\n",
        "plt.hist(q, bins = 100, label='quan')\n",
        "plt.title(\"Quantized model weights of conv1\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YQ7S0iIb8cz"
      },
      "source": [
        "### RECOMMENDATIONS FOR YOUR WORKFLOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-cxT3iwb9jU"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1ij7GElwiKFNmJ221aKpkDRwMj5i-2DLh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBbyE61acqr6"
      },
      "source": [
        "\n",
        "\n",
        "*   Large (10M+ parameters) models are more robust to quantization error\n",
        "*   Quantizing a model from a FP32 checkpoint provides better accuracy than training an INT8 model from scratch\n",
        "*   Dynamic Quantization is an easy first step, especially if your model has many Linear or Recurrent layers\n",
        "*   Use symmetric-per-channel quantization with MinMax observers for quantizing weights. Use affine-per-tensor quantization with MovingAverageMinMax observers for quantizing activations\n",
        "*   Use metrics like SQNR to identify which layers are most suscpetible to quantization error. Turn off quantization on these layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZkMfDNCp9f1"
      },
      "source": [
        "Eager mode https://pytorch.org/docs/stable/quantization.html#eager-mode-quantization\n",
        "\n",
        "\n",
        "\n",
        "*   https://pytorch.org/docs/stable/quantization.html#post-training-dynamic-quantization\n",
        "*   https://pytorch.org/docs/stable/quantization.html#post-training-static-quantization\n",
        "\n",
        "\n",
        "FX Graph https://pytorch.org/docs/stable/quantization.html#prototype-fx-graph-mode-quantization\n",
        "\n",
        "\n",
        "\n",
        "*   https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html\n",
        "*   https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za9ziysYh-7V"
      },
      "source": [
        "## New forkflow with torchao\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHc-7SBJiWid",
        "outputId": "d1eb9403-74f3-4f7d-ef4f-89b91e911618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchao in /usr/local/lib/python3.12/dist-packages (0.10.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchao"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LP_lwV2iGXF"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "\n",
        "class ToyLinearModel(torch.nn.Module):\n",
        "    def __init__(self, m: int, n: int, k: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = torch.nn.Linear(m, n, bias=False)\n",
        "        self.linear2 = torch.nn.Linear(n, k, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "model = ToyLinearModel(1024, 1024, 1024).eval()\n",
        "\n",
        "# Optional: compile model for faster inference and generation\n",
        "model = torch.compile(model, mode=\"max-autotune\", fullgraph=True)\n",
        "model_f32 = copy.deepcopy(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cYxfwkt-xrX"
      },
      "outputs": [],
      "source": [
        "from torchao.quantization import Int4DynamicActivationInt4WeightConfig, quantize_\n",
        "quantize_(model, Int4DynamicActivationInt4WeightConfig())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbGjMaxGj20y",
        "outputId": "1d0f4b33-52f6-430d-8f6f-2eb26fe701f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f32 mean time: 0.193 ms\n",
            "int4 mean time: 0.073 ms\n",
            "speedup: 2.7x\n"
          ]
        }
      ],
      "source": [
        "from torchao.utils import (\n",
        "    benchmark_model,\n",
        "    unwrap_tensor_subclass,\n",
        ")\n",
        "\n",
        "num_runs = 100\n",
        "torch._dynamo.reset()\n",
        "example_inputs = (torch.randn(1, 1024, dtype=torch.float32),)\n",
        "f32_time = benchmark_model(model_f32, num_runs, example_inputs)\n",
        "int4_time = benchmark_model(model, num_runs, example_inputs)\n",
        "\n",
        "print(\"f32 mean time: %0.3f ms\" % f32_time)\n",
        "print(\"int4 mean time: %0.3f ms\" % int4_time)\n",
        "print(\"speedup: %0.1fx\" % (f32_time / int4_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWDUskVC-H1z",
        "outputId": "b98b4661-6fcc-47df-bba6-f93485528489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "int4 model size: 1.01 MB\n",
            "f32 model size: 8.00 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "torch.save(model, \"/tmp/int4_model.pt\")\n",
        "torch.save(model_f32, \"/tmp/f32_model.pt\")\n",
        "int4_model_size_mb = os.path.getsize(\"/tmp/int4_model.pt\") / 1024 / 1024\n",
        "f32_model_size_mb = os.path.getsize(\"/tmp/f32_model.pt\") / 1024 / 1024\n",
        "\n",
        "print(\"int4 model size: %.2f MB\" % int4_model_size_mb)\n",
        "\n",
        "print(\"f32 model size: %.2f MB\" % f32_model_size_mb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI8kSIybQcEQ"
      },
      "source": [
        "### Neural Network Compression Framework (NNCF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7q1Gh6YQfhu",
        "outputId": "fbe50491-0db6-4999-ecd6-5e2df7801f09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'nncf'...\n",
            "remote: Enumerating objects: 89023, done.\u001b[K\n",
            "remote: Counting objects: 100% (1253/1253), done.\u001b[K\n",
            "remote: Compressing objects: 100% (838/838), done.\u001b[K\n",
            "remote: Total 89023 (delta 857), reused 428 (delta 415), pack-reused 87770 (from 2)\u001b[K\n",
            "Receiving objects: 100% (89023/89023), 66.65 MiB | 24.28 MiB/s, done.\n",
            "Resolving deltas: 100% (59899/59899), done.\n",
            "Filtering content: 100% (149/149), 32.33 MiB | 13.72 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/openvinotoolkit/nncf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7inenttGuOti",
        "outputId": "9421e1e3-fea1-4f1b-e284-c468c5f1f79d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/nncf\n",
            "Processing /content/nncf\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from nncf==2.19.0.dev0+9faf5592d) (4.25.1)\n",
            "Requirement already satisfied: natsort>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from nncf==2.19.0.dev0+9faf5592d) (8.4.0)\n",
            "Collecting networkx<3.5.0,>=2.6 (from nncf==2.19.0.dev0+9faf5592d)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting ninja<1.14,>=1.10.0.post2 (from nncf==2.19.0.dev0+9faf5592d)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy<2.3.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from nncf==2.19.0.dev0+9faf5592d) (2.0.2)\n",
            "Collecting openvino-telemetry>=2023.2.0 (from nncf==2.19.0.dev0+9faf5592d)\n",
            "  Downloading openvino_telemetry-2025.2.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from nncf==2.19.0.dev0+9faf5592d) (25.0)\n",
            "Requirement already satisfied: pandas<2.4,>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from nncf==2.19.0.dev0+9faf5592d) (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from nncf==2.19.0.dev0+9faf5592d) (5.9.5)\n",
            "Requirement already satisfied: pydot<=3.0.4,>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nncf==2.19.0.dev0+9faf5592d) (3.0.4)\n",
            "Collecting pymoo>=0.6.0.1 (from nncf==2.19.0.dev0+9faf5592d)\n",
            "  Downloading pymoo-0.6.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: rich>=13.5.2 in /usr/local/lib/python3.12/dist-packages (from nncf==2.19.0.dev0+9faf5592d) (13.9.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from nncf==2.19.0.dev0+9faf5592d) (0.6.2)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from nncf==2.19.0.dev0+9faf5592d) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from nncf==2.19.0.dev0+9faf5592d) (1.16.2)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from nncf==2.19.0.dev0+9faf5592d) (0.9.0)\n",
            "\u001b[33mWARNING: nncf 2.19.0.dev0+9faf5592d does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->nncf==2.19.0.dev0+9faf5592d) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->nncf==2.19.0.dev0+9faf5592d) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->nncf==2.19.0.dev0+9faf5592d) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->nncf==2.19.0.dev0+9faf5592d) (0.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4,>=1.1.5->nncf==2.19.0.dev0+9faf5592d) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4,>=1.1.5->nncf==2.19.0.dev0+9faf5592d) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4,>=1.1.5->nncf==2.19.0.dev0+9faf5592d) (2025.2)\n",
            "Requirement already satisfied: pyparsing>=3.0.9 in /usr/local/lib/python3.12/dist-packages (from pydot<=3.0.4,>=1.4.1->nncf==2.19.0.dev0+9faf5592d) (3.2.5)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.12/dist-packages (from pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.12/dist-packages (from pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d) (1.8.0)\n",
            "Collecting cma>=3.2.2 (from pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d)\n",
            "  Downloading cma-4.4.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting alive-progress (from pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d)\n",
            "  Downloading alive_progress-3.3.0-py3-none-any.whl.metadata (72 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d) (0.3.8)\n",
            "Collecting Deprecated (from pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.5.2->nncf==2.19.0.dev0+9faf5592d) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.5.2->nncf==2.19.0.dev0+9faf5592d) (2.19.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->nncf==2.19.0.dev0+9faf5592d) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->nncf==2.19.0.dev0+9faf5592d) (3.6.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.5.2->nncf==2.19.0.dev0+9faf5592d) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d) (11.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.4,>=1.1.5->nncf==2.19.0.dev0+9faf5592d) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=3.2.0->nncf==2.19.0.dev0+9faf5592d) (4.15.0)\n",
            "Collecting about-time==4.2.1 (from alive-progress->pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d)\n",
            "  Downloading about_time-4.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting graphemeu==0.7.2 (from alive-progress->pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d)\n",
            "  Downloading graphemeu-0.7.2-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting wrapt<2,>=1.10 (from Deprecated->pymoo>=0.6.0.1->nncf==2.19.0.dev0+9faf5592d)\n",
            "  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openvino_telemetry-2025.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading pymoo-0.6.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m139.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cma-4.4.0-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m303.8/303.8 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alive_progress-3.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
            "Downloading graphemeu-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nncf\n",
            "  Building wheel for nncf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nncf: filename=nncf-2.19.0.dev0+9faf5592d-py3-none-any.whl size=1483075 sha256=4539073178f2b404ce7f3316ba7cdfc0dbc3fc8bcc452e3033b512bdc7e3b18d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v9xl0_fz/wheels/ec/bf/bd/19a8710a6d2870ddb4197995c1a9fea9c3714e04c999b5b6fa\n",
            "Successfully built nncf\n",
            "Installing collected packages: openvino-telemetry, wrapt, ninja, networkx, graphemeu, cma, about-time, Deprecated, alive-progress, pymoo, nncf\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 2.0.0\n",
            "    Uninstalling wrapt-2.0.0:\n",
            "      Successfully uninstalled wrapt-2.0.0\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.5\n",
            "    Uninstalling networkx-3.5:\n",
            "      Successfully uninstalled networkx-3.5\n",
            "Successfully installed Deprecated-1.2.18 about-time-4.2.1 alive-progress-3.3.0 cma-4.4.0 graphemeu-0.7.2 networkx-3.4.2 ninja-1.13.0 nncf-2.19.0.dev0+9faf5592d openvino-telemetry-2025.2.0 pymoo-0.6.1.5 wrapt-1.17.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "7fda2f90a3bc4bb7bf399b3d9c7dde02",
              "pip_warning": {
                "packages": [
                  "networkx"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "%cd nncf\n",
        "!pip install .[torch]\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwLu99qZ3IEl",
        "outputId": "1aa66499-1bd5-4df1-bdb5-58a4c0e02889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:nncf:NNCF provides best results with torch==2.9.*, while current torch version is 2.8.0+cu126. If you encounter issues, consider switching to torch==2.9.*\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import nncf.torch  # Important - must be imported before any other external package that depends on torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwXZ_Te7El1C"
      },
      "source": [
        "https://github.com/openvinotoolkit/nncf/tree/develop\n",
        "https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Quantization.md\n",
        "https://github.com/openvinotoolkit/nncf/blob/develop/examples/torch/classification/configs/quantization/inception_v3_imagenet_int8.json\n",
        "\n",
        "https://dev-discuss.pytorch.org/t/torch-ao-quantization-migration-plan/2810\n",
        "https://docs.pytorch.org/ao/stable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysq3_CdFoNuu"
      },
      "source": [
        "** **:  Post Training Quantization   float32 torch2     .        .          (.  ).         SENSITIVITY .      .     / ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI4eVeumGLP_",
        "outputId": "042a416d-c3c9-403e-c8a1-defa6a6eed08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ResNet18...\n",
            " ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/livanoff/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/livanoff/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1,  loss: 0.9129\n",
            " 2,  loss: 0.6255\n",
            " 3,  loss: 0.5878\n",
            "\n",
            " !\n",
            "\n",
            "   ...\n",
            "  : 80.26%\n",
            "   ...\n",
            "  : 796.40 \n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "data_folder = \"./data\"\n",
        "batch_size = 64\n",
        "num_calib_batches = 50\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR10(\n",
        "    root=data_folder, train=True, download=True, transform=transform\n",
        ")\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=data_folder, train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "calib_size = 1024\n",
        "test_size = 5000\n",
        "\n",
        "calib_data, _ = random_split(train_data, [calib_size, len(train_data) - calib_size])\n",
        "test_data, _ = random_split(test_data, [test_size, len(test_data) - test_size])\n",
        "\n",
        "calib_loader = DataLoader(calib_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"  ResNet18...\")\n",
        "model = resnet18(pretrained=True)\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    print(f\" {epoch+1},  loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "model.eval()\n",
        "model = model.cpu()\n",
        "float_model = copy.deepcopy(model)\n",
        "\n",
        "def check_accuracy(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "def check_speed(model, data_loader, num_batches=50):\n",
        "    model.eval()\n",
        "    times = []\n",
        "    for i in range(10):\n",
        "        images, _ = next(iter(data_loader))\n",
        "        _ = model(images)\n",
        "    \n",
        "    for i in range(num_batches):\n",
        "        images, _ = next(iter(data_loader))\n",
        "        \n",
        "        start = time.time()\n",
        "        _ = model(images)\n",
        "        end = time.time()\n",
        "        \n",
        "        times.append(end - start)\n",
        "    \n",
        "    avg_time = sum(times) / len(times) * 1000  #  \n",
        "    return avg_time\n",
        "\n",
        "print(\"\\n   ...\")\n",
        "original_accuracy = check_accuracy(float_model, test_loader)\n",
        "print(f\"  : {original_accuracy:.2f}%\")\n",
        "\n",
        "print(\"   ...\")\n",
        "original_speed = check_speed(float_model, test_loader)\n",
        "print(f\"  : {original_speed:.2f} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "  ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_16258/1372822852.py:15: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared_model = prepare_fx(model_to_quantize, qconfig_dict, example_input)\n",
            "/home/livanoff/miniforge3/lib/python3.12/site-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/home/livanoff/miniforge3/lib/python3.12/site-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_16258/1372822852.py:26: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = convert_fx(prepared_model)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  ...\")\n",
        "\n",
        "import torch.ao.quantization as tq\n",
        "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
        "\n",
        "qconfig = tq.get_default_qconfig(\"fbgemm\")\n",
        "\n",
        "model_to_quantize = copy.deepcopy(float_model)\n",
        "model_to_quantize.eval()\n",
        "\n",
        "example_input = next(iter(calib_loader))[0][:1]  #  \n",
        "qconfig_dict = {\"\": qconfig}\n",
        "\n",
        "prepared_model = prepare_fx(model_to_quantize, qconfig_dict, example_input)\n",
        "\n",
        "print(\" ...\")\n",
        "with torch.no_grad():\n",
        "    count = 0\n",
        "    for images, _ in calib_loader:\n",
        "        prepared_model(images)\n",
        "        count += 1\n",
        "        if count >= num_calib_batches:\n",
        "            break\n",
        "\n",
        "quantized_model = convert_fx(prepared_model)\n",
        "\n",
        "print(\"  ...\")\n",
        "quantized_accuracy = check_accuracy(quantized_model, test_loader)\n",
        "print(f\"  : {quantized_accuracy:.2f}%\")\n",
        "\n",
        "quantized_speed = check_speed(quantized_model, test_loader)\n",
        "print(f\"  : {quantized_speed:.2f} \")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"   ...\")\n",
        "\n",
        "layer_names = []\n",
        "for name, module in float_model.named_modules():\n",
        "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "        layer_names.append(name)\n",
        "\n",
        "layer_names = layer_names[:10]\n",
        "results = []\n",
        "\n",
        "for layer_name in layer_names:\n",
        "    print(f\"\\n : {layer_name}\")\n",
        "    \n",
        "    test_model = copy.deepcopy(float_model)\n",
        "    test_model.eval()\n",
        "    \n",
        "    example_input = next(iter(calib_loader))[0][:1]\n",
        "    \n",
        "    qconfig_dict = {\n",
        "        \"\": None,  #    \n",
        "        \"module_name\": [(layer_name, qconfig)],  #    \n",
        "    }\n",
        "    \n",
        "    prepared = prepare_fx(test_model, qconfig_dict, example_input)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        count = 0\n",
        "        for images, _ in calib_loader:\n",
        "            prepared(images)\n",
        "            count += 1\n",
        "            if count >= num_calib_batches:\n",
        "                break\n",
        "    \n",
        "    quantized = convert_fx(prepared)\n",
        "    \n",
        "    acc = check_accuracy(quantized, test_loader)\n",
        "    drop = original_accuracy - acc\n",
        "    \n",
        "    print(f\"  : {acc:.2f}%\")\n",
        "    print(f\"   : {drop:.2f}%\")\n",
        "    \n",
        "    results.append({\n",
        "        \"\": layer_name,\n",
        "        \"\": acc,\n",
        "        \"\": drop\n",
        "    })\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"   (  ):\")\n",
        "\n",
        "results.sort(key=lambda x: x[\"\"], reverse=True)\n",
        "\n",
        "for i, result in enumerate(results[:5]):\n",
        "    print(f\"{i+1}. {result['']}:  {result['']:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"    3   ...\")\n",
        "\n",
        "skip_layers = [results[0][\"\"], results[1][\"\"], results[2][\"\"]]\n",
        "print(f\" : {skip_layers}\")\n",
        "\n",
        "final_model = copy.deepcopy(float_model)\n",
        "final_model.eval()\n",
        "\n",
        "example_input = next(iter(calib_loader))[0][:1]\n",
        "\n",
        "qconfig_dict = {\n",
        "    \"\": qconfig,  #    \n",
        "    \"module_name\": [(name, None) for name in skip_layers],  #    \n",
        "}\n",
        "\n",
        "prepared_final = prepare_fx(final_model, qconfig_dict, example_input)\n",
        "\n",
        "with torch.no_grad():\n",
        "    count = 0\n",
        "    for images, _ in calib_loader:\n",
        "        prepared_final(images)\n",
        "        count += 1\n",
        "        if count >= num_calib_batches:\n",
        "            break\n",
        "\n",
        "final_quantized = convert_fx(prepared_final)\n",
        "\n",
        "print(\"\\n  ...\")\n",
        "final_accuracy = check_accuracy(final_quantized, test_loader)\n",
        "final_speed = check_speed(final_quantized, test_loader)\n",
        "\n",
        "print(f\"  : {final_accuracy:.2f}%\")\n",
        "print(f\"  : {final_speed:.2f} \")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\" :\")\n",
        "print(f\"{'':<30} {' (%)':<15} {' ()':<15}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'':<30} {original_accuracy:<15.2f} {original_speed:<15.2f}\")\n",
        "print(f\"{' ':<30} {quantized_accuracy:<15.2f} {quantized_speed:<15.2f}\")\n",
        "print(f\"{'  ':<30} {final_accuracy:<15.2f} {final_speed:<15.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQS3K2P-PYwU",
        "outputId": "5c64e89d-2b97-4355-edcb-993b041d107a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Baseline FP32 ===\n",
            "Baseline acc=0.8020, latency=7471.23 ms/batch\n",
            "\n",
            "=== PTQ: default qconfig ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:171: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1721922755.py:181: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PTQ default acc=0.7674, latency=3665.70 ms/batch\n",
            "\n",
            "=== PTQ: HistogramObserver + per-channel weights ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:171: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/tmp/ipython-input-1721922755.py:181: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PTQ hist acc=0.7150, latency=3645.54 ms/batch\n",
            "\n",
            "=== Sensitivity analysis (one layer at a time) ===\n",
            "[*] Sensitivity: quantizing only layer conv1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:215: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1721922755.py:225: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.7790, drop=0.0230\n",
            "[*] Sensitivity: quantizing only layer layer1.0.conv1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:215: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1721922755.py:225: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.7964, drop=0.0056\n",
            "[*] Sensitivity: quantizing only layer layer1.0.conv2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:215: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1721922755.py:225: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8034, drop=-0.0014\n",
            "[*] Sensitivity: quantizing only layer layer1.1.conv1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:215: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1721922755.py:225: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8028, drop=-0.0008\n",
            "[*] Sensitivity: quantizing only layer layer1.1.conv2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:215: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1721922755.py:225: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8012, drop=0.0008\n",
            "[*] Sensitivity: quantizing only layer layer2.0.conv1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:215: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1721922755.py:225: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8010, drop=0.0010\n",
            "[*] Sensitivity: quantizing only layer layer2.0.conv2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:215: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1721922755.py:225: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8024, drop=-0.0004\n",
            "[*] Sensitivity: quantizing only layer layer2.0.downsample.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:215: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1721922755.py:225: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8036, drop=-0.0016\n",
            "[*] Sensitivity: quantizing only layer layer2.1.conv1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:215: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1721922755.py:225: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8020, drop=0.0000\n",
            "[*] Sensitivity: quantizing only layer layer2.1.conv2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:215: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1721922755.py:225: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acc=0.8018, drop=0.0002\n",
            "\n",
            "Top sensitive layers (  accuracy):\n",
            "                   layer     acc    drop\n",
            "0                  conv1  0.7790  0.0230\n",
            "1         layer1.0.conv1  0.7964  0.0056\n",
            "5         layer2.0.conv1  0.8010  0.0010\n",
            "4         layer1.1.conv2  0.8012  0.0008\n",
            "9         layer2.1.conv2  0.8018  0.0002\n",
            "8         layer2.1.conv1  0.8020  0.0000\n",
            "6         layer2.0.conv2  0.8024 -0.0004\n",
            "3         layer1.1.conv1  0.8028 -0.0008\n",
            "2         layer1.0.conv2  0.8034 -0.0014\n",
            "7  layer2.0.downsample.0  0.8036 -0.0016\n",
            "\n",
            "  (  FP32) : ['conv1', 'layer1.0.conv1', 'layer2.0.conv1']\n",
            "\n",
            "=== PTQ with sensitivity (skip sensitive layers) ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1721922755.py:255: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared = prepare_fx(model, qconfig_dict, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  prepared = prepare(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1721922755.py:265: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized = convert_fx(prepared)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PTQ (skip sensitive) acc=0.7914, latency=4828.26 ms/batch\n",
            "\n",
            "=== Summary: accuracy / latency ===\n",
            "                                variant        type  sensitivity_used     acc  \\\n",
            "0                         FP32 baseline        FP32             False  0.8020   \n",
            "1                   PTQ default_qconfig  static PTQ             False  0.7674   \n",
            "2                 PTQ HistogramObserver  static PTQ             False  0.7150   \n",
            "3  PTQ default_qconfig + skip sensitive  static PTQ              True  0.7914   \n",
            "\n",
            "   latency_ms_per_batch  \n",
            "0           7471.232760  \n",
            "1           3665.695359  \n",
            "2           3645.543031  \n",
            "3           4828.259954  \n"
          ]
        }
      ],
      "source": [
        "import torch.ao.quantization as tq\n",
        "\n",
        "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
        "\n",
        "all_results = []\n",
        "\n",
        "#   \n",
        "print(\"   (FP32)...\")\n",
        "base_acc = check_accuracy(float_model, test_loader)\n",
        "base_speed = check_speed(float_model, test_loader)\n",
        "\n",
        "all_results.append({\n",
        "    \"\": \" FP32\",\n",
        "    \"\": \"FP32\",\n",
        "    \" \": \"\",\n",
        "    \" (%)\": round(base_acc, 2),\n",
        "    \" ()\": round(base_speed, 2)\n",
        "})\n",
        "\n",
        "print(f\": {base_acc:.2f}%, : {base_speed:.2f} \")\n",
        "\n",
        "#      \n",
        "quantization_methods = [\n",
        "    {\n",
        "        \"\": \" \",\n",
        "        \"\": \"PTQ    \",\n",
        "        \"\": \"default\"\n",
        "    },\n",
        "    {\n",
        "        \"\": \"  \",\n",
        "        \"\": \"PTQ  HistogramObserver\",\n",
        "        \"\": \"histogram\"\n",
        "    }\n",
        "]\n",
        "\n",
        "#    \n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"   ...\")\n",
        "\n",
        "for method in quantization_methods:\n",
        "    print(f\"\\n{method['']}: {method['']}\")\n",
        "    \n",
        "    if method[''] == \"default\":\n",
        "        qconfig = tq.get_default_qconfig(\"fbgemm\")\n",
        "    else:\n",
        "        qconfig = tq.QConfig(\n",
        "            activation=tq.HistogramObserver.with_args(\n",
        "                dtype=torch.quint8,\n",
        "                qscheme=torch.per_tensor_affine\n",
        "            ),\n",
        "            weight=tq.PerChannelMinMaxObserver.with_args(\n",
        "                dtype=torch.qint8,\n",
        "                qscheme=torch.per_channel_symmetric\n",
        "            ),\n",
        "        )\n",
        "    \n",
        "    model_copy = copy.deepcopy(float_model)\n",
        "    model_copy.eval()\n",
        "    \n",
        "    example_input = next(iter(calib_loader))[0][:1]\n",
        "    prepared = prepare_fx(model_copy, {\"\": qconfig}, example_input)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        count = 0\n",
        "        for images, _ in calib_loader:\n",
        "            prepared(images)\n",
        "            count += 1\n",
        "            if count >= 30:\n",
        "                break\n",
        "    \n",
        "    quantized_model = convert_fx(prepared)\n",
        "    \n",
        "    acc = check_accuracy(quantized_model, test_loader)\n",
        "    speed = check_speed(quantized_model, test_loader)\n",
        "    \n",
        "    print(f\"  : {acc:.2f}%\")\n",
        "    print(f\"  : {speed:.2f} \")\n",
        "    \n",
        "    all_results.append({\n",
        "        \"\": f\"PTQ: {method['']}\",\n",
        "        \"\": \"PTQ\",\n",
        "        \" \": \"\",\n",
        "        \" (%)\": round(acc, 2),\n",
        "        \" ()\": round(speed, 2)\n",
        "    })\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  ...\")\n",
        "\n",
        "sensitivity_results = []\n",
        "for layer_name in layer_names:\n",
        "    print(f\" : {layer_name}\")\n",
        "    \n",
        "    test_model = copy.deepcopy(float_model)\n",
        "    test_model.eval()\n",
        "    \n",
        "    example_input = next(iter(calib_loader))[0][:1]\n",
        "   \n",
        "    qconfig = tq.get_default_qconfig(\"fbgemm\")\n",
        "    qconfig_dict = {\n",
        "        \"\": None,\n",
        "        \"module_name\": [(layer_name, qconfig)],\n",
        "    }\n",
        "    prepared = prepare_fx(test_model, qconfig_dict, example_input)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        count = 0\n",
        "        for images, _ in calib_loader:\n",
        "            prepared(images)\n",
        "            count += 1\n",
        "            if count >= 20:\n",
        "                break\n",
        "    \n",
        "    quantized = convert_fx(prepared)\n",
        "    \n",
        "    acc = check_accuracy(quantized, test_loader)\n",
        "    drop = base_acc - acc\n",
        "    \n",
        "    print(f\"  : {acc:.2f}%, : {drop:.2f}%\")\n",
        "    \n",
        "    sensitivity_results.append({\n",
        "        \"\": layer_name,\n",
        "        \"\": acc,\n",
        "        \"\": drop\n",
        "    })\n",
        "\n",
        "sensitivity_results.sort(key=lambda x: x[\"\"], reverse=True)\n",
        "\n",
        "print(\"\\n-5   :\")\n",
        "for i, result in enumerate(sensitivity_results[:5]):\n",
        "    print(f\"{i+1}. {result['']}:  {result['']:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"     ...\")\n",
        "\n",
        "num_to_skip = min(3, len(sensitivity_results))\n",
        "skip_layers = [result[\"\"] for result in sensitivity_results[:num_to_skip]]\n",
        "\n",
        "print(f\" : {skip_layers}\")\n",
        "\n",
        "final_model = copy.deepcopy(float_model)\n",
        "final_model.eval()\n",
        "\n",
        "qconfig = tq.get_default_qconfig(\"fbgemm\")\n",
        "qconfig_dict = {\n",
        "    \"\": qconfig,\n",
        "    \"module_name\": [(name, None) for name in skip_layers],\n",
        "}\n",
        "\n",
        "example_input = next(iter(calib_loader))[0][:1]\n",
        "prepared = prepare_fx(final_model, qconfig_dict, example_input)\n",
        "\n",
        "with torch.no_grad():\n",
        "    count = 0\n",
        "    for images, _ in calib_loader:\n",
        "        prepared(images)\n",
        "        count += 1\n",
        "        if count >= 30:\n",
        "            break\n",
        "\n",
        "final_quantized = convert_fx(prepared)\n",
        "\n",
        "final_acc = check_accuracy(final_quantized, test_loader)\n",
        "final_speed = check_speed(final_quantized, test_loader)\n",
        "\n",
        "print(f\": {final_acc:.2f}%\")\n",
        "print(f\": {final_speed:.2f} \")\n",
        "\n",
        "all_results.append({\n",
        "    \"\": \"PTQ   \",\n",
        "    \"\": \"PTQ\",\n",
        "    \" \": \"\",\n",
        "    \" (%)\": round(final_acc, 2),\n",
        "    \" ()\": round(final_speed, 2)\n",
        "})\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\" :\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "for i in range(1, len(results_df)):\n",
        "    acc_diff = results_df.loc[i, \" (%)\"] - results_df.loc[0, \" (%)\"]\n",
        "    speed_diff = results_df.loc[i, \" ()\"] - results_df.loc[0, \" ()\"]\n",
        "    speed_ratio = results_df.loc[i, \" ()\"] / results_df.loc[0, \" ()\"]\n",
        "    \n",
        "    print(f\"\\n{results_df.loc[i, '']}:\")\n",
        "    print(f\"  : {results_df.loc[i, ' (%)']:.2f}% \"\n",
        "          f\"({'+' if acc_diff > 0 else ''}{acc_diff:.2f}%)\")\n",
        "    print(f\"  : {results_df.loc[i, ' ()']:.2f}  \"\n",
        "          f\"({'+' if speed_diff > 0 else ''}{speed_diff:.2f} , \"\n",
        "          f\"{speed_ratio:.1f}x {'' if speed_ratio < 1 else ''})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  :\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\":\")\n",
        "\n",
        "best_model = None\n",
        "best_score = -float('inf')\n",
        "\n",
        "for i, row in enumerate(all_results[1:]):\n",
        "    acc_score = (row[\" (%)\"] / base_acc) * 70\n",
        "    speed_score = (base_speed / row[\" ()\"]) * 30\n",
        "    \n",
        "    total_score = acc_score + speed_score\n",
        "    \n",
        "    if total_score > best_score:\n",
        "        best_score = total_score\n",
        "        best_model = row[\"\"]\n",
        "\n",
        "print(f\" : {best_model}\")\n",
        "print(\"(     )\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
